\documentclass[a4pape,oneside,10pt]{article}
\usepackage{graphicx}
\usepackage[level]{datetime}
% \usepackage{CJK}
\usepackage[UTF8]{ctex}
\usepackage{amsmath}
% \newdateformat{ukdate}{\THEYEAR.\monthname[\THEMONTH].\ordinaldate{\THEDAY}}
\newdateformat{ukdate}{\THEYEAR.\THEMONTH.\THEDAY}

\begin{document}
\title{VINS中的边缘化}
\author{Wang Changlong}
\date{\ukdate\today}
\maketitle
%section
\section{什么是边缘化}
边缘化，总结为一句话就是从联合概率分布中求边际概率。即
\begin{equation}
    P(X=x) = \int_{ -\infty}^{\infty}  P(X=x, Y=y) dy
\end{equation}
具体到vins系统中，为了维护变量的维度可控以达到实时性，我们常常维护一个固定长度的滑窗。当窗口进行滑动时，某些旧的变量需要被舍弃，
这个舍弃的过程就牵扯到了边缘化。

考虑一个长度为4的变量窗口，在$t=2$的时刻，$X=[x_0, x_1, x_2]$。$t=3$时来一个新的变量$x_3$，我们把$x_3$加入到变量窗口里，
并对变量$X_{old}=[x_0, x_1, x_2, x_3]$进行估计，这时我们得到了关于变量$x_i, i=1,2,3,4$的联合概率分布。而后，我们舍弃掉变量$x_0$，
而保留关于变量$X_{new}=[x_1, x_2, x_3]$的联合概率分布，这个舍弃就是通过边缘化来实现的。
\begin{equation}
\begin{aligned}
    P(X_{new}=x_{new}) 
    &= \int_{ -\infty}^{\infty}  P(X_{old}=[x_{new}, \tilde{x}_{0}]) d{\tilde{x}_0} \\
    &= \int_{ -\infty}^{\infty}  P(X_{new}=x_{new}, x_0=\tilde{x}_{0}) d{\tilde{x}_0}
\end{aligned}
\end{equation}

\section{协方差矩阵、信息矩阵和边缘化}
假设变量$(x,y)$的概率密度函数符合均值\textbf{$\mu$}，协方差\textbf{$\Sigma$}的高斯分布，具有如下的矩阵形式：
\begin{equation}
p(x,y) = \mathcal{N} (
    \begin{bmatrix}
        \textbf{$\mu_x$} \\
        \textbf{$\mu_y$}
    \end{bmatrix},
    \begin{bmatrix}
        \textbf{$\Sigma_{xx}$} & \textbf{$\Sigma_{xy}$} \\
        \textbf{$\Sigma_{yx}$} & \textbf{$\Sigma_{yy}$}
    \end{bmatrix} 
    )
\end{equation}
那么边缘化就非常好操作了，对于变量$y$，其对应的子块$\mu_y$和$\Sigma_{yy}$就是边缘化掉变量$x$后，$y$的均值和协方差。

但是在很多基于优化的vins中，我们对于变量的分布是以信息矩阵$\varLambda$和信息向量$\eta$的形式给出的，具有如下形式：
\begin{equation}
    p(x,y) = \mathcal{N} (
        \varLambda^{-1}
        \begin{bmatrix}
            \textbf{$\eta_x$} \\
            \textbf{$\eta_y$}
        \end{bmatrix},
        \begin{bmatrix}
            \textbf{$\varLambda_{xx}$} & \textbf{$\varLambda_{xy}$} \\
            \textbf{$\varLambda_{yx}$} & \textbf{$\varLambda_{yy}$}
        \end{bmatrix}^{-1}
        )
\end{equation}
这时候我们想进行边缘化得到变量$y$的信息矩阵就没有那么直观了。舒尔补分解协方差矩阵并求逆，可以得到信息矩阵和协方差矩阵的关系：
\begin{equation}
    \begin{split}
        \begin{aligned}
    \begin{bmatrix}
        \textbf{$\varLambda_{xx}$} & \textbf{$\varLambda_{xy}$} \\
        \textbf{$\varLambda_{yx}$} & \textbf{$\varLambda_{yy}$}
    \end{bmatrix}
    &=
    \begin{bmatrix}
        \textbf{$\Sigma_{xx}$} & \textbf{$\Sigma_{xy}$} \\
        \textbf{$\Sigma_{yx}$} & \textbf{$\Sigma_{yy}$}
    \end{bmatrix} ^{-1} \\
    &=
    \begin{bmatrix}
        \textbf{1} & \textbf{0} \\
        \textbf{-$\Sigma_{yy}^{-1}\Sigma_{yx}$} & \textbf{1}
    \end{bmatrix}     
    \begin{bmatrix}
        \textbf{$(\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})^{-1}$} & \textbf{0} \\
        \textbf{0} & \textbf{$\Sigma_{yy}^{-1}$}
    \end{bmatrix}
    \begin{bmatrix}
        \textbf{1} & \textbf{-$\Sigma_{xy}\Sigma_{yy}^{-1}$} \\
        \textbf{0} & \textbf{1}
    \end{bmatrix}  \\
    &=
    \begin{bmatrix}
        \textbf{$(\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})^{-1}$} & \textbf{$-(\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})^{-1}\Sigma_{xy}\Sigma_{yy}^{-1}$} \\
        \textbf{$-\Sigma_{yy}^{-1}\Sigma_{yx}(\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})^{-1}$} & \textbf{$\Sigma_{yy}^{-1} + \Sigma_{yy}^{-1}\Sigma_{yx}(\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx})^{-1}\Sigma_{xy}\Sigma_{yy}^{-1}$}
    \end{bmatrix}    
\end{aligned}
\end{split}
\end{equation}
由此可得，变量$y$的信息矩阵，即协方差$\Sigma_{yy}$的逆为：
\begin{equation}
    \label{info}
    \begin{split}
        \begin{aligned}
    \Sigma_{yy}^{-1} = \textbf{$\varLambda_{yy} - \varLambda_{yx}\varLambda_{xx}^{-1}\varLambda_{xy}$}
\end{aligned}
\end{split}
\end{equation}

我们知道，对于上述高斯分布，我们可以采用\textbf{舒尔补}进行分解，得到边缘概率密度函数和条件概率密度函数。
\begin{equation}
\label{schur}
\begin{split}
\begin{aligned}
    p(x,y) &= p(x|y)p(y) \\
    p(x|y) &= \mathcal{N}(\mu_x + \Sigma_{xy}\Sigma_{yy}^{-1}(y-\mu_y), \Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}^{-1}\Sigma_{yx}) \\
    p(y) &= \mathcal{N} (\mu_y, \Sigma_{yy})
\end{aligned}
\end{split}
\end{equation}
这时我们对照公式Eq(\ref{schur})和公式Eq(\ref{info})，会发现条件概率$P(x|y)$的协方差就是$\varLambda_{xx}^{-1}$！也就是说，我们如果直接从信息矩阵中取块，得到的是变量的条件概率分布的信息矩阵。

这引出了一个非常有趣的现象：边际概率对于协方差矩阵的操作是很容易的，对于信息矩阵不好操作，而条件概率恰好相反。

\section{边缘化的平方根方法}
上节中我们得到，直接对信息矩阵进行舒尔补分解是可以实现边缘化的，这一节我们学习边缘化的平方根（Square Root）方法。
平方根方法的引入源自在求解非线性最小二乘问题时，信息矩阵通常是形如$H=J^TJ$来进行构造的，如高斯牛顿方法和LM方法。如果我们对$J$进行QR分解，有
\begin{equation}
    \label{qr}
    \begin{split}
    \begin{aligned}
        \textbf{J} = \begin{bmatrix}
            \textbf{Q}_1 & \textbf{Q}_2
        \end{bmatrix} ^T
        \begin{bmatrix}
            \textbf{R} \\ \textbf{0}
        \end{bmatrix},  
        \quad with \quad \textbf{R} = \begin{bmatrix}
            R & T \\
            0 & P
        \end{bmatrix}
    \end{aligned}
    \end{split}
    \end{equation}
那么对于信息矩阵有
\begin{equation}
    \label{squareroot}
    \begin{split}
    \begin{aligned}
        \textbf{H} = \textbf{R}^T\textbf{R} = 
        \begin{bmatrix}
            R & T \\
            0 & P
        \end{bmatrix} ^T
        \begin{bmatrix}
            R & T \\
            0 & P
        \end{bmatrix}
        =
        \begin{bmatrix}
            R^TR & R^TT \\
            T^TR & P^TP + T^TT
        \end{bmatrix} ^T
    \end{aligned}
    \end{split}
\end{equation}
对照公式Eq(\ref{info})和公式Eq(\ref{squareroot})，我们发现恰好有$\Sigma_{yy}^{-1} = P^TP$。
对于QR分解，我们知道，它得到的是一个正交矩阵和一个上三角矩阵的乘积。因此我们想要边缘化的子变量部分必须要放在变量向量的头部，才能直接使用上述方法进行边缘化。

实际上，我们进行边缘化的时候并不需要真的要求出正交矩阵$\textbf{Q}$，我们的目的是将求出矩阵$R$，而Givens Rotation告诉我们，只需要不断地左乘Givens旋转矩阵，即可将消去矩阵$\textbf{J}$的下三角中的非零元素。
\end{document}